// Copyright 2023 Peter Hebert. Licensed under the MIT license.

// Package internal contains non-exported implementation details of gotoken.
package internal

import (
	"fmt"
	"regexp"
	"strings"

	"github.com/peterheb/gotoken"
)

// BPETokenizer implements the [gotoken.Tokenizer] interface. All tokenizers
// returned by [gotoken.GetTokenizer] are BPETokenizers.
//
// Internally, BPETokenizer uses a combination of a splitting function and a BPE
// dictionary. The splitting function divides the text to tokenize into semantic
// parts based on character classes (typically keeping similar character classes
// together and breaking on white space or punctuation in some consistent way.)
// The BPE dictionary then encodes frequently encountered sequences of bytes
// into their own tokens.
//
// To create the canonical tokenization of a given sequence of bytes, the
// following steps are performed:
//
//   - Convert the source bytes into their corresponding tokens.
//   - Determine which pairs of adjacent tokens are encodable as a higher-numbered
//     token.
//   - Of those, choose the lowest-ranked pair (the one with the lowest resulting
//     token #), and combine those two tokens into a new token.
//   - Repeat this process until no more pairs can be combined.
//
// Working this out for "Bonjour" in r50k_base results in the following steps:
//
//   - "Bonjour" == []byte{66, 111, 110, 106, 111, 117, 114}
//   - Equivalent tokens == []int{33, 78, 77, 73, 78, 84, 81}
//   - {33, 78, 77, 73, 78, 84, 81} == {"B", "o", "n", "j", "o", "u", "r"}
//   - {33, 261, 73, 78, 84, 81} == {"B", "on", "j", "o", "u", "r"}
//   - {33, 261, 73, 280, 81} == {"B", "on", "j", "ou", "r"}
//   - {33, 261, 73, 454} == {"B", "on", "j", "our"}
//   - {20682, 73, 454} == {"Bon", "j", "our"}
//
// Combining tokens in a different order (for example, strictly left-to-right)
// could result in a different tokenization of the same word. For example, a
// valid tokenization of "Bonjour" is:
//
//   - {20682, 7639, 333} == {"Bon", "jo", "ur"}
//
// Calling Decode() on any of the above token arrays returns "Bonjour". However,
// non-canonical tokenizations may not be the same length as the ones generated
// by OpenAI's APIs.
type BPETokenizer struct {
	params                *BPEParams
	disallowSpecialTokens bool           // if true, special tokens return an error
	allowedSpecialTokens  map[string]int // map of allowed special tokens for encoding
	decodeSpecialTokens   map[int]string // map of all special tokens, for decoding
	specialTokenRegex     *regexp.Regexp // regular expression that matches ALL special tokens
}

// higherThanAnyToken is a placeholder value that is higher than any token in
// any of our supported encodings.
const higherThanAnyToken = 0x7fffffff

// BPEParams contains the parameters defining the encoding used by a
// BPETokenizer. These are the data structures generated by gen.go.
type BPEParams struct {
	Name           string
	Splitter       func([]byte) [][]byte
	ByteEncoder    []byte         // token values for each byte 0-255
	EncoderTrie    serializedTrie // pseudo-map[string]int for strings->tokens
	DecoderMap     []string       // strings for each token int
	SpecialTokens  map[string]int // map of all defined special tokens
	BytePairLookup []int          // lookup table for byte pairs, 256*256 entries
}

// NewBPETokenizer creates a new BPETokenizer from the given BPEParams and using
// the specified special token encoding settings.
func NewBPETokenizer(params *BPEParams, allowSpecialAsText bool, allowedSpecialTokens []string) (*BPETokenizer, error) {
	ret := BPETokenizer{
		params:                params,
		disallowSpecialTokens: !allowSpecialAsText,
		allowedSpecialTokens:  make(map[string]int),
		decodeSpecialTokens:   make(map[int]string),
	}

	// Initialization for special tokens (specialTokenRegex, decodeSpecialTokens)
	var parts []string
	for k := range params.SpecialTokens {
		parts = append(parts, regexp.QuoteMeta(k))
		ret.decodeSpecialTokens[params.SpecialTokens[k]] = k
	}
	ret.specialTokenRegex = regexp.MustCompile("(" + strings.Join(parts, "|") + ")")

	// Fill allowedSpecialTokens if appropriate
	if len(allowedSpecialTokens) > 0 {
		for _, k := range allowedSpecialTokens {
			if tok, ok := params.SpecialTokens[k]; ok {
				ret.allowedSpecialTokens[k] = tok
			} else {
				return nil, fmt.Errorf("special token %q not found in tokenizer %q", k, params.Name)
			}
		}
	}

	return &ret, nil
}

// Encode converts a string into a slice of ints (tokens). A wrapped
// [gotoken.ErrSpecialToken] error will be returned if a special token appears
// in the input without being explicitly allowed.
func (tt *BPETokenizer) Encode(s string) ([]int, error) {
	// Special token disallow check
	if err := tt.Allowed(s); err != nil {
		return nil, err
	}

	// Return value (preallocate len/4 tokens as a heuristic)
	encoded := make([]int, 0, len(s)/4+1)
	input := []byte(s)

	// Loop until we've consumed all of the input
	for len(input) > 0 {
		// segment contains what to encode-- by default it's all of input
		segment := input
		var specialMatch []int

		if tt.specialTokenRegex != nil {
			// If a special token is found, limit segment to just up until the
			// special token. We'll BPE that []byte, encode the special token,
			// and loop.
			specialMatch = tt.specialTokenRegex.FindIndex(input)
			if specialMatch != nil {
				segment = input[:specialMatch[0]]
			}
		}

		// Split the segment into parts, and encode each part
		parts := tt.params.Splitter(segment)
		for _, part := range parts {
			if len(part) <= 2 {
				// handle one and two byte tokens using lookup tables
				if len(part) == 1 {
					// encode one byte directly to its token
					encoded = append(encoded, int(tt.params.ByteEncoder[part[0]]))
				} else if twoTok := tt.params.BytePairLookup[int(part[0])<<8|int(part[1])]; twoTok != -1 {
					// len(part)==2: try to encode the byte pair using BytePairLookup
					encoded = append(encoded, twoTok)
				} else {
					// len(part)==2 && twoTok==-1: encode the individual bytes as tokens
					encoded = append(encoded, int(tt.params.ByteEncoder[part[0]]), int(tt.params.ByteEncoder[part[1]]))
				}
				continue
			}

			// If the whole part is a token, just encode it directly.
			if wholeTok := tt.params.EncoderTrie.Lookup(part); wholeTok != -1 {
				encoded = append(encoded, wholeTok)
				continue
			}

			// Slower path: perform BPE on part and output returned tokens
			encoded = append(encoded, tt.applyBPE(part)...)
		}

		if specialMatch != nil {
			// Was there a special token? If so, encode it and continue
			foundToken := input[specialMatch[0]:specialMatch[1]]
			if tokenNum, ok := tt.allowedSpecialTokens[string(foundToken)]; ok {
				// if approved, emit as special
				encoded = append(encoded, tokenNum)
			} else {
				// otherwise, emit as text
				encoded = append(encoded, tt.applyBPE(foundToken)...)
			}
			// Consume the segment we processed plus the special token, and loop
			input = input[specialMatch[1]:]
			continue
		} else {
			// If no special token, all input has now been consumed, so break
			break
		}
	}

	return encoded, nil
}

// Allowed performs the special token safety check on an input string according
// to the configuration of this Tokenizer. A wrapped [gotoken.ErrSpecialToken]
// is returned if the input contains a special token defined by this encoding
// that has not been not explicitly allowed.
//
// If a Tokenizer instance was created with the [gotoken.AllowSpecialAsText]
// option, this method will always return no error (nil).
func (tt *BPETokenizer) Allowed(input string) error {
	if tt.disallowSpecialTokens && tt.specialTokenRegex != nil {
		matches := tt.specialTokenRegex.FindAllString(input, -1)
		for _, match := range matches {
			if _, ok := tt.allowedSpecialTokens[match]; !ok {
				return fmt.Errorf("%w: %q", gotoken.ErrSpecialToken, match)
			}
		}
	}
	return nil
}

// Decode converts a slice of ints (tokens) into a string. It may return an
// error that wraps [gotoken.ErrInvalidToken] if any of the provided tokens are
// not valid in this encoding.
func (tt *BPETokenizer) Decode(tokens []int) (string, error) {
	var ret strings.Builder

	maxToken := len(tt.params.DecoderMap) - 1
	for _, token := range tokens {
		if token < 0 || token > maxToken {
			spc, ok := tt.decodeSpecialTokens[token]
			if !ok {
				return "", fmt.Errorf("%w: %d", gotoken.ErrInvalidToken, token)
			}
			ret.WriteString(spc)
			continue
		}
		ret.WriteString(tt.params.DecoderMap[token])
	}
	return ret.String(), nil
}

// Count returns the number of tokens in an input string, without returning the
// actual tokens. It returns 0 if the input string is empty, or if the input
// cannot be encoded.
func (tt *BPETokenizer) Count(input string) int {
	tokens, err := tt.Encode(input)
	if err != nil {
		return 0
	}
	return len(tokens)
}

// tokenInfo tracks information about a token in the BPE algorithm.
type tokenInfo struct {
	token    int
	start    int
	length   int
	prevIdx  int
	nextIdx  int
	thisPair int
}

// applyBPE applies the BPE algorithm to the given input string, and returns the
// resulting []int. This is intended to be run on a substring that has already
// been split out of the input string. This method is not exposed via the
// Tokenizer interface and is for internal use by gotoken.
func (tt *BPETokenizer) applyBPE(input []byte) []int {
	// early exit when encoding empty input
	count := len(input)
	if count == 0 {
		return nil
	}

	// set up a doubly linked list of tokens
	tokens := make([]tokenInfo, count)
	for i, b := range input {
		tokens[i] = tokenInfo{
			token:   int(tt.params.ByteEncoder[b]),
			start:   i,
			length:  1,
			prevIdx: i - 1,
			nextIdx: i + 1,
		}
	}
	tokens[count-1].nextIdx = -1

	// and populate the initial token pairings
	pairTokens := make([]int, len(input)-1)
	for i := 0; i < len(pairTokens); i++ {
		tokens[i].thisPair = tt.params.BytePairLookup[int(input[i])<<8|int(input[i+1])]
	}
	tokens[count-1].thisPair = -1

	// main tokenization loop
	var mergeIdx int
	trie := tt.params.EncoderTrie
	for {
		minTokenRank := higherThanAnyToken
		mergeIdx = -1

		// find the lowest-ranked pair in pairTokens
		i := 0
		for {
			nextIdx := tokens[i].nextIdx
			if nextIdx == -1 {
				// end of linked list, no pairs left to evaluate
				break
			}
			if tokens[i].thisPair != -1 && tokens[i].thisPair < minTokenRank {
				minTokenRank = tokens[i].thisPair
				mergeIdx = i
			}
			i = nextIdx
		}

		// no pairs left to merge, exit loop
		if mergeIdx == -1 {
			break
		}

		// perform the merge and update our data structures
		nextIdx := tokens[mergeIdx].nextIdx
		tokens[mergeIdx].token = minTokenRank
		tokens[mergeIdx].length += tokens[nextIdx].length

		// remove the deleted token from the linked list
		tokens[mergeIdx].nextIdx = tokens[nextIdx].nextIdx
		nextIdx = tokens[mergeIdx].nextIdx
		if nextIdx != -1 {
			tokens[nextIdx].prevIdx = mergeIdx
		}
		count--

		// update thisPair values reflect the new possible pairs created by the merge
		prevIdx := tokens[mergeIdx].prevIdx
		if prevIdx != -1 {
			tokens[prevIdx].thisPair = trie.Lookup(input[tokens[prevIdx].start : tokens[prevIdx].start+tokens[prevIdx].length+tokens[mergeIdx].length])
		}
		if nextIdx != -1 {
			tokens[mergeIdx].thisPair = trie.Lookup(input[tokens[mergeIdx].start : tokens[mergeIdx].start+tokens[mergeIdx].length+tokens[nextIdx].length])
		}
	}

	ret := make([]int, count)
	dest := 0
	idx := 0
	for idx != -1 {
		ret[dest] = tokens[idx].token
		idx = tokens[idx].nextIdx
		dest++
	}
	return ret
}
